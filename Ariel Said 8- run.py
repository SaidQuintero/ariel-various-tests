import os
import json
import re
from datetime import datetime

from langchain_openai import AzureOpenAIEmbeddings
from azure.core.credentials import AzureKeyCredential
from azure.search.documents import SearchClient
from azure.search.documents.models import QueryType
from azure.search.documents._generated.models import QueryCaptionResult

from openai import AzureOpenAI
import tiktoken

from dotenv import load_dotenv

load_dotenv()

def num_tokens_from_string(string: str, encoding_name: str = "cl100k_base") -> int:
    """Returns the number of tokens in a text string."""
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

class AzureEmbeddings:

    def __init__(self):
        pass

    @staticmethod
    def get_embedding():
        return AzureOpenAIEmbeddings(
            azure_deployment=os.getenv("OPENAI_AZURE_DEPLOYMENT"), 
            openai_api_version="2023-08-01-preview",
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            azure_endpoint=os.getenv("OPEN_AI_AZURE_URL")
        )

    @staticmethod
    def generate_embeddings(content: str):
        embeddings = AzureOpenAIEmbeddings(
            azure_deployment=os.getenv("OPENAI_AZURE_DEPLOYMENT"), 
            openai_api_version="2023-08-01-preview",
            openai_api_key=os.getenv("OPENAI_API_KEY"),
            azure_endpoint=os.getenv("OPEN_AI_AZURE_URL")
        )

        doc_result = embeddings.embed_documents([content])

        return doc_result[0]

openai_client = AzureOpenAI(
    api_key=os.getenv("AZURE_CHAT_OPENAI_API_KEY"), 
    api_version=os.getenv("AZURE_CHAT_OPENAI_API_VERSION"), 
    azure_endpoint=os.getenv("AZURE_CHAT_OPENAI_ENDPOINT")
    )

embeddings_client = AzureEmbeddings()
store_search_url: str = f'https://{os.getenv('AZURE_COGNITIVE_SEARCH_SERVICE_NAME')}.search.windows.net'
search_client = SearchClient(
            store_search_url, os.getenv("AZURE_COGNITIVE_SEARCH_INDEX_NAME"),
            AzureKeyCredential(os.getenv("AZURE_COGNITIVE_SEARCH_API_KEY"))
        )

log_file_name = ""


def ensure_directory(directory_name):
    if not os.path.exists(directory_name):
        os.makedirs(directory_name)


def log(message):
    global log_file_name
    ensure_directory("logs")
    with open(log_file_name, "a") as file:
        file.write(f"{datetime.now()}\n{message}\n")
        
        
conversation_price = 0
conversation_ct = 0
conversation_pt = 0
conversation_tt = 0


def get_conversation_price(new_completion, model="gpt4o"):
    global conversation_price, conversation_ct, conversation_pt, conversation_tt
    USD_price_in_COP = 4100

    if model == "gpt4o-mini":
        ct_price = 0.0000006
        pt_price = 0.00000015
    if model == "gpt4o":
        ct_price = 0.000015
        pt_price = 0.000005

    ct = new_completion.usage.completion_tokens
    pt = new_completion.usage.prompt_tokens
    tt = new_completion.usage.total_tokens
    usd_total_price = (ct * ct_price) + (pt * pt_price)
    cop_total_price = usd_total_price * USD_price_in_COP
    conversation_ct += ct
    conversation_pt += pt
    conversation_tt += tt
    conversation_usd_total_price = (conversation_ct * ct_price) + (
        conversation_pt * pt_price
    )
    conversation_cop_total_price = conversation_usd_total_price * USD_price_in_COP

    print(
        f"üí∞ Conversation price: ${conversation_cop_total_price} COP (In: {conversation_pt}, Out: {conversation_ct}, Total token: {conversation_tt})\n"
        f"   This message: ${cop_total_price} COP (In: {pt}, Out: {ct}, Total token: {tt}) "
        f""
    )
    log(
        f"üí∞ Conversation price: ${conversation_cop_total_price} COP (In: {conversation_pt}, Out: {conversation_ct}, Total token: {conversation_tt})\n "
        f"   This message: ${cop_total_price} COP (In: {pt}, Out: {ct}, Total token: {tt}) "
        f""
    )

    return

### üîë Definir system prompt y tools

messages = []
conversation = []
user_input = ""

codes_to_laws = "\n- C√≥digo Penal: Ley 599 de 2000\n- C√≥digo Civil: Ley 57 de 1887\n- C√≥digo de Comercio: Decreto 410 de 1971\n- C√≥digo de Procedimiento Civil: Decreto 1400 de 1970\n- C√≥digo de Procedimiento Penal: Ley 906 de 2004\n- C√≥digo General del Proceso: Ley 1564 de 2012\n- C√≥digo de la Infancia y la Adolescencia: Ley 1098 de 2006\n- C√≥digo Nacional de Polic√≠a: Decreto 1355 de 1970\n- C√≥digo de Recursos Naturales: Decreto 2811 de 1974\n- C√≥digo Electoral: Decreto 2241 de 1986\n- C√≥digo Disciplinario √önico: Ley 734 de 2002\n- C√≥digo Contencioso Administrativo: Ley 1437 de 2011 (anteriormente Decreto 1 de 1984)\n- C√≥digo de Minas: Ley 685 de 2001\n- C√≥digo de Educaci√≥n: Ley 115 de 1994\n- C√≥digo Nacional de Tr√°nsito Terrestre: Ley 769 de 2002\n- C√≥digo del Menor: Decreto 2737 de 1989\n- C√≥digo de Construcci√≥n del Distrito Capital de Bogot√°: Acuerdo 20 de 1995\n- C√≥digo de Construcci√≥n Sismo-Resistente: Acuerdo correspondiente (no especificado en los resultados)\n- C√≥digo de R√©gimen Departamental: Decreto 1222 de 1986\n- C√≥digo de R√©gimen Pol√≠tico y Municipal: Decreto-Ley 1333 de 1986\n- C√≥digo Penal Militar: Ley 522 de 1999\n- C√≥digo Penitenciario y Carcelario: Ley 65 de 1993\n- C√≥digo Procesal del Trabajo y del Seguro Social: Decreto-Ley 2158 de 1948\n- C√≥digo Sustantivo del Trabajo: Decreto 2663 de 1950\n- C√≥digo Sanitario Nacional: Ley 9 de 1979\n- C√≥digo General Disciplinario: Ley 1952 de 2019\n- C√≥digo Nacional de Seguridad y Convivencia Ciudadana: Ley 1801 de 2016\n- CODIGO DE POLICIA DE BOGOTA: ACUERDO 79 DE 2003\n- C√≥digo Penal Militar: Ley 522 de 1999\n- C√≥digo Penitenciario y Carcelario: Ley 65 de 1993\n- C√≥digo Procesal del Trabajo y del Seguro Social: Decreto-Ley 2158 de 1948\n- C√≥digo Sustantivo del Trabajo: Decreto 2663 de 1950\n- C√≥digo Sanitario Nacional: Ley 9 de 1979\n- C√≥digo General Disciplinario: Ley 1952 de 2019\n- C√≥digo Nacional de Seguridad y Convivencia Ciudadana: Ley 1801 de 2016"


get_next_chunk = {
    "type": "function",
    "function": {
        "name": "get_next_chunk",
        "description": "Use this tool to get when an important sources comes incomplete or the content is cut off. This tool will help you retrieve the following section of that given source.",
        "parameters": {
            "type": "object",
            "properties": {
                "source_id": {
                    "type": "string",
                    "description": "the unique identifier of the chunk that is incomplete. eg:'20240719192458csjscpboletinjurisprudencial20181219pdf_chunk30'",
                },
            },
            "required": ["source_id"],
        },
    },
}

response_system_template = f"""
Eres Ariel, un asistente para la investigaci√≥n legal en jurisprudencia colombiana. Sigue estas instrucciones al responder:

1. Consulta de Fuentes:
- Tienes acceso a algunas fuentes que previasmente has encontrado y seleccionado para generar esta respuesta. - Dentro de esas fuentes se encuentra toda la informaci√≥n disponible para responder. 
- Expresa toda la informaci√≥n que encuentres en las fuentes proporcionadas y que sea relevante para responder.
- Si las fuentes no ayudan a responder la pregunta, responde: "No encuentro informaci√≥n con esos t√©rminos, ¬øpuedes reformular tu consulta?"

2. Redacci√≥n de Respuestas:
- S√© detallado y preciso, utilizando exclusivamente la informaci√≥n de las fuentes proporcionadas.
- No incluyas informaci√≥n que no haya sido extra√≠da directamente de las fuentes.

3. Citaci√≥n de Fuentes:
- Cita el mayor n√∫mero de fuentes aplicables.
- Utiliza corchetes para referenciar la fuente con el ID del fragmento sin modificarlo, por ejemplo: [12345_id_de_ejemplo].
- No combines fuentes; lista cada fuente por separado.

3. Formato de Respuesta:
- Escribe tus respuestas en formato HTML, sin incluir los tags "```html" al principio o al final.
- Utiliza etiquetas HTML est√°ndar como <p>, <strong>, <em>, etc.

5. Interpretaci√≥n de Categor√≠as:
- Prioriza las fuentes seg√∫n su categor√≠a:
    1. Constitucional: Es la norma m√°xima y m√°s importante establecida por el gobierno del pa√≠s. 
    2. Legal: Es la que le sigue en importancia y son los decretos, c√≥digos y leyes oficiales del pa√≠s establecida por los ministerios. 
    3. Infralegal: Le sigue en importancia a la categor√≠a Legal. 
    4. Jurisprudencia: Son los documentos de juicios reales y de c√≥mo se ha ejercido la ley en el pasado. 
    5. Doctrina: Son documentos o libros escritos por autores del derecho que no son parte de la ley pero ayudan a darle una interpetaci√≥n a la ley.
- Da mayor importancia a las fuentes m√°s actuales.

6. Precisi√≥n Legal:
- Siempre dale m√°s importancia a la fuente primaria. Si debes citar una ley, art√≠culo o norma, cita la fuente directa, es decir, esa ley, art√≠culo o normal.
- Las fuentes secundarias te ayudar√°n a entender la interpretaci√≥n sobre las fuentes primarias.
- No confundas leyes o art√≠culos; si el usuario pregunta por una ley espec√≠fica, ignora extractos de otras leyes.
- Utiliza las fuentes para dar una respuesta completa y darle al usuario informaci√≥n adicional que pueda serle de ayuda.
- Cita textualmente cuando necesites mencionar el contenido un art√≠culo, ley u otro documento y si el tama√±o de este lo permite.

7. Evita Imprecisiones:
- En caso de encontrar informaci√≥n contradictoria, se√±√°lala y sugiere una posible causa.
- No te salgas del tema de la pregunta del usuario.
- En caso que el usuario haya dado informaci√≥n incorrecto o imprecisa, se√±√°lala y da un argumento.
- Si la ley, la jurisprudencia y la doctrina tienen respuestas diferentes o interpretaciones diferentes para la misma pregunta, se√±ala las distintas perspectivas.
"""

# Incluir que el usuario puede estar proporcionando alguna informaci√≥n que es falsa, en ese caso debe apuntarla y argumentar con la informaci√≥n de las fuentes el porqu√© es falsa.
# Incluir que se√±ale informaci√≥n contradictoria si es relevante en las fuentes.
# Que siempre le d√© m√°s importancia a las fuentes primarias.
# Que las fuentes secundarias ayudan a dar interpretaci√≥n y contexto.

query_system_template = f"""Eres un experto legal que necesita buscar en una base de conocimiento con cientos de miles de documentos, fuentes confiables y precisas para responder las consultas del usuario. La base de conocimientos est√° alojada en Azure AI Search y bebes generar un query por cada b√∫squeda que necesitas y que que conserve todo el significado sem√°ntico de la idea. 

Los documentos est√°n guardados en fragmentos con los siguientes campos:
- id: Identificador √∫nico del fragmento
- title: T√≠tulo del documento
- author: Autor del documento
- keywords: Tema legal, puede ser: General, Constitucional, Internacional_Publico, Internacional_Privado, Penal, Financiero, entre otros.
- category: Tipo de documento legal. Las opciones son: Jurisprudencia, Doctrina, Constitucional, Legal, Infralegal y Otros_temas_legales. 
- page: P√°gina de la que fue extra√≠do el fragmento.
- year: A√±o en el que se public√≥ el documento.
- content: 500 tokens de contenido, fragmento o extracto del documento. Son solo 500 tokens del documento completo.

Pasos para generar la b√∫squeda:
1. Identifica qu√© informaci√≥n necesitas investigar para responder de forma precisa y completa al usuario.
2. Solo puedes hacer 3 b√∫squedas, as√≠ que elabora un plan de investigaci√≥n organizar√°s c√≥mo puedes distribuir tus b√∫squedas en caso que necesites hacer m√°s de 1.  
3. Genera un query por cada b√∫squedad que necesitas ejecutar. Puedes generar un m√°ximo de 3 b√∫squedas. Puedes buscar cualquier cosa, desde teor√≠as del derecho, hasta leyes o una b√∫squeda general.
4. Cada b√∫squeda solo puede contener 1 idea. Eso significa que si debes comparar conceptos, leyes o documentos, debes buscarlos por separado.
5. Por cada b√∫squeda, da una breve explicaci√≥n donde agumentes porqu√© necesitas hacer esa b√∫squeda.

Cada query debe cumplir con las siguientes condiciones:
- Debe ser una idea sem√°nticamente completa y contener los keywords relevantes.
- No debe ser demasiado largo. S√© conciso, pero nunca dejes por fuera conceptos claves como el lugar, momento u otros aspectos claves.
- Siempre incluye el qu√©, cu√°ndo o c√≥mo en tu t√©rmino de b√∫squeda, ya esto har√° que la b√∫squeda est√© sem√°nticamente completa y genere los resultados correctos. Es decir, si por ejemplo el usuario pregunta '¬øCu√°ndo se ...' el query debe incluir la palabra 'cu√°ndo', 'momento de' o algo que haga similar.
- Si el usuario pregunta por el momento, lugar o alguna condici√≥n, esto debe estar presente en tu query.
- Si identificas una fuente espec√≠fica como el n√∫mero de ley, art√≠culo, decreto, el t√≠tulo del documento, el nombre del autor o el tipo de documento legal que necesitas buscar, debes incluirlo en el query.
- Si hay varias formas de referise a una fuente, incluye los diferentes nombres de esa fuente en el query.
- No es necesario que todas tus b√∫squedas sean espec√≠ficas para un documento, recuerda que solo puedes hacer m√°ximo 3 b√∫squedas, as√≠ que en ocasiones puedes obtener mejor informaci√≥n si buscas conceptos m√°s generales en vez de un documento espec√≠fico.

Esta es una lista de los c√≥digos en colombia y sus respectiva ley. Si necesitas buscar alguna de estas fuentes, debes incluir ambas refencias en tu query: {codes_to_laws}
"""


query_schema = {
    "type": "json_schema",
    "json_schema": {
        "name": "search_query",
        "strict": True,
        "schema": {
            "type": "object",
            "properties": {
                "searches": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "search_query": {
                                "type": "string",
                                "description": "El query de b√∫squeda completo. Este debe cumplir con todos los requitiso.",
                            },
                            "reason": {
                                "type": "string",
                                "description": "Argumento de porqu√© necesitas hacer esta b√∫squeda.",
                            },
                        },
                        "required": [
                            "search_query",
                            "reason",
                        ],
                        "additionalProperties": False,
                    },
                }
            },
            "required": ["searches"],
            "additionalProperties": False,
        },
    },
}
selected_sources_schema = {
    "type": "json_schema",
    "json_schema": {
        "name": "selected_sources",
        "strict": True,
        "schema": {
            "type": "object",
            "properties": {
                "sources": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "properties": {
                            "source_id": {
                                "type": "string",
                                "description": "ID de la fuente que seleccionaste",
                            },
                            "reason": {
                                "type": "string",
                                "description": "Argumento de porqu√© seleccionaste esta fuente.",
                            },
                        },
                        "required": [
                            "source_id",
                            "reason",
                        ],
                        "additionalProperties": False,
                    },
                }
            },
            "required": ["sources"],
            "additionalProperties": False,
        },
    },
}
# Agregar un campo para identificar fuentes cortadas

def generate_completion(completion_messages):
    log(f"{json.dumps(completion_messages, indent=4)}\n")
    response = openai_client.chat.completions.create(
        model=os.getenv("AZURE_CHAT_OPENAI_DEPLOYMENT"),
        messages=completion_messages,
        tools=[get_next_chunk],
        temperature=0.2,
        n=1,
        tool_choice="auto",
    )
    log(response)
    get_conversation_price(response)
    return response


def generate_mini_completion(completion_messages):
    log(f"{json.dumps(completion_messages, indent=4)}\n")
    response = openai_client.chat.completions.create(
        model=os.getenv("AZURE_CHAT_MINI_OPENAI_DEPLOYMENT"),
        messages=completion_messages,
        temperature=0.2,
        n=1,
    )
    log(response)
    get_conversation_price(response, model="gpt4o-mini")
    return response


def choose_sources_completion(completion_messages):
    log(f"{json.dumps(completion_messages, indent=4)}\n")
    response = openai_client.chat.completions.create(
        model=os.getenv("AZURE_CHAT_MINI_OPENAI_DEPLOYMENT"),
        messages=completion_messages,
        response_format=selected_sources_schema,
        temperature=0.0,
        n=1,
    )
    log(response)
    get_conversation_price(response, model="gpt4o-mini")
    return response


def generate_query(query_messages):
    log(f"{json.dumps(query_messages, indent=4)}\n")
    response = openai_client.chat.completions.create(
        model=os.getenv("AZURE_CHAT_MINI_OPENAI_DEPLOYMENT"),
        messages=query_messages,
        response_format=query_schema,
        temperature=0.0,
    )
    log(response)
    get_conversation_price(response, model="gpt4o-mini")
    return response

def format_search_results(docs_list):
    print(f"üìã Agarrando todos los resultados ...")
    documents = []
    try:
        docs_list = list(docs_list)
        print(f"üìã Formateando resultados ...")
        for index, document in enumerate(docs_list, start=1):
            captions: QueryCaptionResult = document.get("@search.captions", "")
            captions_text = " // ".join([caption.text for caption in captions]) if captions is not None else ""
            doc_formatted = {
                "position" : index,
                "score": document.get("@search.score", 10),
                "rerank": document.get("@search.reranker_score", 10),
                "captions": captions_text,
                "id": document["id"],
                "title": document["title"],
                "author": document["author"],
                "keywords": document["keywords"],
                "category": document["category"],
                "page": document["page"],
                "year": document["year"],
                "has_copyright": document["has_copyright"],
                "file_path": document["file_path"],
                "external_id": document["external_id"],
                "content": document["content"],
            }
            documents.append(doc_formatted)
    except Exception as e:
        print(e)
    log(
        f">>>>>>format_search_results:\n"
        f"{json.dumps(documents, indent=4)}"
        )
    return documents

def filter_docs(doc_list):
    print(f"üßπ Eliminando fuentes irrelevantes o duplicadas ...")
    filtered_docs = []
    seen_docs = set()
    
    for doc in doc_list:
        score = doc.get("score", 10) 
        rerank = doc.get("rerank", 10) 
        content = doc["content"] 
        if (
            content not in seen_docs
            and (rerank > 2
            or score > 0.17)
        ):
            filtered_docs.append(doc)
            seen_docs.add(content)
    print(f"üßπ Docs filtrados. De {len(doc_list)} pasaron {len(filtered_docs)} ...")
    log(f"üßπ Docs filtrados. De {len(doc_list)} pasaron {len(filtered_docs)} ...\n"
    f"{json.dumps(filtered_docs, indent=4)}")
    return filtered_docs

def add_doc_to_context(docs_list):
    print(f"üìã Agregando fuentes al contexto ...")
    sources = ""
    counter = 0
    
    for document in docs_list:
        counter += 1
        sources += (
            f"Fuente:\n"
            f"{document.get("reason", "")}"
            f"id: {document["id"]}\n"
            f"title: {document["title"]}\n"
            f"author: {document["author"]}\n"
            f"year: {document["year"]}\n"
            f"keywords: {document["keywords"]}\n"
            f"category: {document["category"]}\n"
            f"page: {document["page"]}\n"
            f"content: {document["content"]}\n"
            f"\n\n"
        )      
    log(
        f"To add in Context: \n"
        f"{json.dumps({"sources":sources}, indent=4)}"
        )
    print(f"üìã {counter} fuentes agregadas ...")
    return sources
    
    
def search_for_chunks(text_query):
    log(f"üîé Buscando documentos: {text_query} ...")
    print(f"üîé Buscando documentos: {text_query} ...")
    results_num = 50
    vector_query = embeddings_client.generate_embeddings(content=text_query)

    results = search_client.search(
        search_text=text_query,
        vector_queries=[
            {
                "vector": vector_query,
                "k": results_num,
                "fields": "content_vector",
                "kind": "vector",
                "exhaustive": True,
            }
        ],
        top=results_num,
        query_type=QueryType.SEMANTIC,
        semantic_configuration_name="default",
        query_caption="extractive|highlight-false",
        scoring_profile="legal",
        scoring_parameters=[
            "tagx6-Constitucional",
            "tagx5-Legal",
            "tagx4-Infralegal",
            "tagx3-Jurisprudencia",
            "tagx2-Doctrina",
        ],
    )
    results_formatted = format_search_results(results)
    restults_filtered = filter_docs(results_formatted)

    return restults_filtered[:25]


def get_next_chunks(id):
    log(f"‚è©Ô∏è Buscando los chunks siguientes de {id} ...")
    print(f"‚è©Ô∏è Buscando los chunks siguientes de {id} ...")
    pattern = r"^(.*_chunk)(\d+)$"
    match = re.search(pattern, id)

    if match:
        prefix = match.group(1)
        chunk_number = int(match.group(2))
        next_chunks_result = search_client.search(
            filter=f"id eq '{prefix}{chunk_number + 1}' or id eq '{prefix}{chunk_number + 2}'",
            top=2,
        )
        return format_search_results(list(next_chunks_result))


def get_next_chunk_tool(source_id):
    next_chunks = get_next_chunks(source_id)
    log(f"Continuaci√≥n de {source_id}:\n{add_doc_to_context(next_chunks)}")
    return f"Continuaci√≥n de {source_id}:\n{add_doc_to_context(next_chunks)}"

def append_message(new_message):
    global messages, conversation

    if isinstance(new_message, dict):
        role = new_message.get("role")
        content = new_message.get("content")
    else:
        role = new_message.role
        content = new_message.content

    if role == "assistant":
        if new_message.tool_calls:
            tool_calls = new_message.tool_calls
            tool_calls_formatted = []
            for tool_call in tool_calls:
                tool_calls_formatted.append(
                    {
                        "id": tool_call.id,
                        "function": {
                            "arguments": str(json.loads(tool_call.function.arguments)),
                            "name": tool_call.function.name,
                        },
                        "type": "function",
                    }
                )
            messages.append(
                {
                    "role": "assistant",
                    "tool_calls": tool_calls_formatted,
                },
            )
            return messages  ## Assistant call tools
        elif content:
            messages.append({"role": "assistant", "content": content})
            return messages  ## Assistant talk
    elif role == "tool":
        messages.append(new_message)
        return messages  ## Tool reponse
    else:
        return messages
    
    
def call_tools(tool_calls):
    for tool_call in tool_calls:
        tool_name = tool_call.function.name
        tool_args = json.loads(tool_call.function.arguments)
        if tool_name == "get_next_chunk":
            content = get_next_chunk_tool(tool_args.get("source_id"))

        new_tool_message = {
            "tool_call_id": tool_call.id,
            "role": "tool",
            "name": tool_name,
            "content": content,
        }

        append_message(new_message=new_tool_message)

    print(">> ü§ñ Generando respuesta ...")
    new_assistant_completion = generate_completion()
    new_assistant_message = new_assistant_completion.choices[0].message
    append_message(new_message=new_assistant_message)

    if new_assistant_message.content:
        return print("üí¨ Assistant:" + new_assistant_message.content)
    else:
        return call_tools(
            tool_calls=new_assistant_message.tool_calls,
        )
        
def run_conversation(user_prompt=None):
    if user_prompt:
        get_answer(user_prompt)
    else:
        while True:
            user_input = input("You: ")
            if user_input.lower() == "exit":
                print("Exiting chat...")
                break
            get_answer(user_input)


def get_answer(user_prompt):
    global messages, log_file_name, user_input

    user_input = user_prompt

    current_time = datetime.now().strftime("%m-%d %H:%M")
    log_file_name = f"logs/{current_time} - {user_input[:50]}.log"

    print("üôé‚Äç‚ôÇÔ∏è User: " + user_input)
    log(f"User input: {user_input}")

    query_prompt_message = {"role": "system", "content": query_system_template}
    query_user_prompt_message = {
        "role": "user",
        "content": f"Genera uno o varias b√∫squedas para responder esta pregunta: {user_input}",
    }

    query_messages = [
        query_prompt_message,
        *messages,
        query_user_prompt_message,
    ]

    print("ü§ñ Generando queries con la pregunta del usuario ...")
    first_query_completion = generate_query(query_messages)
    first_search_terms = json.loads(
        first_query_completion.choices[0].message.content
    ).get("searches")
    print(f"ü§ñ {len(first_search_terms)} b√∫squedas generadas ...")

    # TODO: Puedo explorar en meterle una validaci√≥n de si necesita hacer otra pasa de b√∫squeda o en los documentos ya est√° la respuesta completa y exacta.

    sources_found = []
    search_result_prompt = ""
    for search in first_search_terms:
        search_query = search.get("search_query")
        query_reason = search.get("reason")
        # print(f"Buscando '{search_query}': {query_reason}")
        print(query_reason)
        query_results = search_for_chunks(search_query)
        sources_found += query_results
        sources_to_add = add_doc_to_context(query_results)
        search_result_prompt += f"{query_reason}.\nResultados de la b√∫squeda '{search_query}':\n{sources_to_add}\n\n"

    sources_prompt_message = {
        "role": "assistant",
        "content": search_result_prompt,
    }
    sources_instructions_prompt_message = {
        "role": "system",
        "content": f"Ejectuaste una b√∫squeda inicial y encontraste algunos extractos con esa b√∫squeda. Ahora, sigue las siguientes instrucciones:\n1. Revisa todos los extractos obtenidos con tus primeras b√∫squedas e identifica fuentes relevantes que te ayuden a responder al usuario.\n2. Identifica leyes, art√≠culos, o nombres de sentencias o documentos exactos e incl√∫yelos en el query si son importantes para responder la pregunta del usuario.\n3. Revisa tu estrategia de b√∫squeda inicial y mej√≥rala. Ten en cuenta que debes separar conceptos, uno por cada b√∫squeda en la medida de los posible.\n4. Arma tu query teniendo en cuenta las fuentes, incluyendo el contexto sem√°ntico completo, pero s√© conciso y no busques varias cosas en un solo query.\n5. Si necesitas buscar m√°s de una fuente o t√©rmino, crea otra b√∫squeda.7. Si la ley, art√≠culo o sentencia tiene un n√∫mero ej: 'Ley 203 de 1999', incl√∫yela en el query junto a su nombre de referencia 'C√≥digo de ...'. 8. Ahora que tienes informaci√≥n a tu disposici√≥n, puedes revisar si la pregunta hecha por el usuario est√° bien formulada o no y corregir las b√∫squedas en base a estos hallazgos. 9. Sigue las condiciones iniciales que debe cumplir cada query.\n10. Identifica si hay menciones a otras fuentes relevantes, teor√≠as o definiciones que necesitas investigar m√°s.\n\n",
    }

    query_messages += [
        sources_prompt_message,
        sources_instructions_prompt_message,
    ]

    print("ü§ñ Generando nuevas b√∫squedas con la nueva informaci√≥n ...")
    second_query_completion = generate_query(query_messages)
    second_search_terms = json.loads(
        second_query_completion.choices[0].message.content
    ).get("searches")
    print(f"ü§ñ {len(second_search_terms)} b√∫squedas generadas ...")

    query_reasons = ""
    for search in second_search_terms:
        search_query = search.get("search_query")
        query_reason = search.get("reason")
        # print(f"Buscando '{search_query}': {query_reason}")
        print(query_reason)
        query_results = search_for_chunks(search_query)
        sources_found += query_results
        query_reasons += f"{query_reason} Por eso busqu√© '{search_query}'. "

    sources_found_filtered = filter_docs(sources_found)
    sources_to_add = add_doc_to_context(sources_found_filtered)

    response_prompt_message = {"role": "system", "content": response_system_template}
    select_user_prompt_message = {
        "role": "user",
        "content": f"Busca informaci√≥n relevante y selecciona las mejores fuentes para responder a: {user_input}",
    }
    select_prompt_message = {
        "role": "assistant",
        "content": f"{query_reasons}.\n\nResultados de mis b√∫squedas:\n{sources_to_add}\n\nEjectuaste una b√∫squeda con lo que el usuario te pregunt√≥ y encontraste fuentes que pueden servirte, ahora debes escoger las mejores. Sigue las siguientes instrucciones:\n1. Revisa todos los extractos obtenidos con tu b√∫squeda y escoge un m√°ximo de 10 fuentes que te sirvan para responder al usuario.\n2. Enlista las fuentes que escogiste e identif√≠calas por su ID y por ning√∫n motivo en absoluto cambies el valor del ID o n√∫mero de chunk de una fuente. Ord√©nalas por importancia, posicionando de primero la que consideres m√°s relevante o m√°s cercanas a lo que el usuario pidi√≥.\n3.(Opcional) Intenta seleccionar un grupo de fuentes donde puedas dar una respuesta completa a la pregunta, se√±alar contradicciones en las fuentes, dar informaci√≥n complementaria relevante y se√±alar diversos puntos de vista al respecto lo que el usuario pregunta.\n4. Al lado de cada fuente que seleccionaste escribe una muy breve explicaci√≥n argumentando porqu√© la escogiste y porqu√© ayuda a responder mejor la solicitud del usuario.\n5. Toda la informaci√≥n debe provenir de lo que dicen las fuentes, no inventes ni des informaci√≥n adicional que no est√©n presentes en las fuentes.\nRecomendaciones adicionales\n- Dale m√°s importancia a las fuentes que son extractos del documento que el usuario pidio.\n- Conforma tu selecci√≥n tanto de fuentes primarias como secundarias si esto ayuda a reponder mejor la pregunta.\n- Ten en cuenta que los nombres de algunos codigos hacen referencia a una ley, decreto o articulo. Aqui tienes un listado para que te guies:\n{codes_to_laws}",
    }
    # select_instructions_prompt_message = {
    #     "role": "system",
    #     "content": f"Ejectuaste una b√∫squeda con lo que el usuario te pregunt√≥ y encontraste fuentes que pueden servirte, ahora debes escoger las mejores. Sigue las siguientes instrucciones:\n1. Revisa todos los extractos obtenidos con tu b√∫squeda y escoge un m√°ximo de 10 fuentes que te sirvan para responder al usuario.\n2. Enlista las fuentes que escogiste e identif√≠calas por su ID y por ning√∫n motivo en absoluto cambies el valor del ID o n√∫mero de chunk de una fuente. Ord√©nalas por importancia, posicionando de primero la que consideres m√°s relevante o m√°s cercanas a lo que el usuario pidi√≥.\n3.(Opcional) Intenta seleccionar un grupo de fuentes donde puedas dar una respuesta completa a la pregunta, se√±alar contradicciones en las fuentes, dar informaci√≥n complementaria relevante y se√±alar diversos puntos de vista al respecto lo que el usuario pregunta.\n4. Al lado de cada fuente que seleccionaste escribe una muy breve explicaci√≥n argumentando porqu√© la escogiste y porqu√© ayuda a responder mejor la solicitud del usuario.\n5. Toda la informaci√≥n debe provenir de lo que dicen las fuentes, no inventes ni des informaci√≥n adicional que no est√©n presentes en las fuentes.\nRecomendaciones adicionales\n- Dale m√°s importancia a las fuentes que son extractos del documento que el usuario pidio.\n- Conforma tu selecci√≥n tanto de fuentes primarias como secundarias si esto ayuda a reponder mejor la pregunta.\n- Ten en cuenta que los nombres de algunos codigos hacen referencia a una ley, decreto o articulo. Aqui tienes un listado para que te guies:\n{codes_to_laws}",
    # }

    selection_messages = [
        response_prompt_message,
        *messages,
        select_user_prompt_message,
        select_prompt_message,
        # select_instructions_prompt_message,
    ]

    print("ü§ñ Escogiendo las mejores fuentes para responder al usuario ...")
    selected_sources_to_add = []
    selected_sources_completion = choose_sources_completion(selection_messages)
    selected_sources = json.loads(
        selected_sources_completion.choices[0].message.content
    ).get("sources")
    for index, source in enumerate(selected_sources, start=1):
        source_id = source.get("source_id")
        source_reason = source.get("reason")
        for source in sources_found_filtered:
            if source["id"] == source_id:
                source["reason"] = source_reason
                selected_sources_to_add.append(source)
        print(f"{index}. [{source_id}] : {source_reason}")

    # return
    sources_to_add = add_doc_to_context(selected_sources_to_add)

    user_message = {
        "role": "user",
        "content": user_input,
    }
    sources_prompt_message = {
        "role": "assistant",
        "content": f"Fuentes econtradas:\n{sources_to_add}",
    }

    response_messages = [
        response_prompt_message,
        *messages,
        user_message,
        sources_prompt_message,
    ]

    response_completion = generate_completion(response_messages)
    print(response_completion.choices[0].message.content)

    # print(">> ü§ñ Generando respuesta ...")
    # new_user_message = {"role": "user", "content": user_input}
    # messages = [new_user_message]
    # response = generate_completion()
    # response_message = response.choices[0].message

    # messages = append_message(new_message=response_message)

    # tool_calls = response_message.tool_calls
    # if tool_calls:
    #     call_tools(tool_calls=tool_calls)
    # elif response_message.content:
    #     print("üí¨ Assistant:", response_message.content)
    # else:
    #     print("An error ocurred during completion generation - response: ", response)
    
run_conversation("¬øQu√© dice el art√≠culo 103 del c√≥digo penal?")